{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sesi√≥n de Estudio N¬™3 WWCode Santiago "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contenidos üìò\n",
    "\n",
    "El objetivo de esta sesi√≥n es que te familiaces con los siguientes conceptos:\n",
    "\n",
    "- Machine Learning\n",
    "- Feature Selection\n",
    "- Accuracy\n",
    "- Overfitting\n",
    "- Matrix de correlaci√≥n\n",
    "\n",
    "## 2. Forma de trabajo üôä\n",
    "\n",
    "En esta oportunidad t√∫ ser√°s tu propia speaker, a trav√©s de este notebook se te plantear√°n una serie de ejercicios y pregunta que deber√°s ir resolviendo. No te preocupes si no alcanzas a terminarlo, es muy probable que hayan m√°s ejercicios de los posibles a hacer durante la jornada. Si tienes dudas o consultas no dudes en preguntar a la monitora de la sesi√≥n o a tus propias compa√±eras, esta instancia est√° hecha para que entre todas aprendamos algo nuevo\n",
    "\n",
    "\n",
    "## 3. Introducci√≥n al tema ü§ñ\n",
    "\n",
    "### Feature selection\n",
    "Es el proceso que permite selecionar de manera m√°s eficiente aquellos atributos del dataset que efectivamente son necesarios para el modelo y descartar aquellos que solo agregan ruido. Entre las ventajas de aplicar feature selection se encuentra:\n",
    "\n",
    "- Reducci√≥n de Overfitting\n",
    "- Mejoras en la precisi√≥n\n",
    "- Reducci√≥n del tiempo de entrenamiento\n",
    "\n",
    "\n",
    "Revisamos 6 t√©cnicas diferentes para feature selection, para ello utilizaremos el dataset reducido del desaf√≠o **[Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk/data)** de Kaggle disponible en el repo. Sin embargo, si quieres usar el dataset original te invito a revisar las intrucciones para habilitar la API de Kaggle en tu equipo\n",
    "\n",
    "> https://github.com/Kaggle/kaggle-api\n",
    "\n",
    "## 4.Manos al a obra üí™üèª\n",
    "\n",
    "A continuaci√≥n Encontrar√°s celdas de c√≥digo con instrucciones que debes seguir. De ahora en adelante el desarrollo es 100% tu creatividad, apoy√°te de textos y gr√°ficos cuando estimes conveniente. Las preguntas que salen son una gu√≠a de lo que debes hacer, pero puedes agregar todo lo que estimes conveniente. Recuerda que tus an√°lisis no siempre los revisan personas expertas y todos debens ser capaces de seguir tu l√≥gica.\n",
    "\n",
    "\n",
    "**_Mucho √©xito üôå !!_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importa las librer√≠as necesarias para tu trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input de datos.\n",
    "#crea una variable llamada datos que lea el archivo application_train_lite.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Revisi√≥n del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Descripci√≥n general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ac√° debes responder a las preguntas tipo:\n",
    "#Cuant√°s columnas tiene el dataset?Cu√°ntas filas?\n",
    "#Cual es la variable objetivo? Spoiler:target.\n",
    "#Esta variables es categ√≥rica?Es data desbalanceada?\n",
    "#La variable es num√©rica?, tiene patrones de comportamiento?, es muy dispersa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tratamiento de missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An√°lisis del dataset\n",
    "#Cu√°ntas variables categoricas hay?Cu√°ntas num√©ricas?\n",
    "#Cu√°l es el % de datos missing?\n",
    "#Genera imputaci√≥n de missing. Ap√≥yate de Imputer de SkLeark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Ning√∫n Criterio\n",
    "\n",
    "Para probar la efectividad de feature selection primero se entrenar√° un modelo con todos los campos y su accuracy se tomar√° como base. Al final de cada criterio de feature selection se volver√° a entrenar el mismo modelo, con el fin de evaluar cambios en su performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrena tu modelo favorito (evita usar el automodel). \n",
    "#Si no tienes ninguno en mente puedes usar un arbol de decisi√≥n simple.\n",
    "#Nota. si sientes que tienes buen manejo de c√≥digo, podr√≠as escribir un m√©todo\n",
    "#Donde entregues el dataset y retorne el accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Filtros\n",
    "\n",
    "M√©todos que eliminan columnas **antes** de que sea necesario entrenar algo. Se conocen por ser m√©todos aplicados 100% en el preproceso\n",
    "\n",
    "##### **3.1.1 Variance Threshold**\n",
    "Considerando que si una variables es relativamente constante no agrega mucho valor a tomar decisiones, este criterio elimina todas aquellas que tengan baja varianza\n",
    "\n",
    "##### **3.1.1 Correlation Threshold**\n",
    "Elimina variables que est√©n fuertemente relacionadas entre s√≠ o que sean muy similares , con el fin de no tener informaci√≥n redundante\n",
    "\n",
    "##### **3.1.1 Univariate selection**\n",
    "Analiza relaci√≥n que existe entre cada variable y el target de manera independiente. Para ello, se pueden utilizar dos test estad√≠sticos\n",
    "\n",
    "- Coeficiente de correlaci√≥n de Pearson: \n",
    "- Chi cuadrado (Usual en variables categ√≥ricas)\n",
    "- ANOVA (Usual en variables num√©ricas)\n",
    "- Mutual Information and maximal information coefficient (MIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Elige 3 criterios de los anteriores, revisa en internet para escribir una breve\n",
    "#definici√≥n y realiza un primer feature selection.\n",
    "#2.Indica qu√© variables quedaron fuera y revisa cu√°nto cambia el accuracy de tu \n",
    "#modelo\n",
    "\n",
    "\n",
    "#Tip: Alabado sea SKlearn, con el nada te faltar√°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Wrapper-based Method\n",
    "Est√°n basado en algoritmos de b√∫squeda que van evaluando todas las posibles combinaciones de variables y seleccionan aquella que produce el mejor resultado de predicci√≥n\n",
    "\n",
    "\n",
    "##### **3.2.1 Forward Search**\n",
    "Inicialmente considera la performance con cada una de las variables de manera separada.La que tenga mejor resultado ser√° considerada como variable inicial y sobre esta se genera la combinatoria de b√∫squeda.\n",
    "\n",
    "\n",
    "##### **3.2.2 Backward Search**\n",
    "Este m√©todo comienza entrenando con todas las variables ingresadas y va eliminando en la medida que sube la performance\n",
    "\n",
    "##### **3.2.3 Recursive Feature Elimination**\n",
    "Similar a un modelo de optimizaci√≥n, busca encontrar el conjunto de variables que maximice el rendimiento del modelo. Para lograr dicho objetivo, crea modelos de forma iterativa, dejando de lado las variables con mejores y peores resultados (es decir, deja las que tienen un rendimiento equilibrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Elige 1 criterios de los anteriores y realiza un segundo feature selection.\n",
    "#2.Indica qu√© variables quedaron fuera y revisa cu√°nto cambia el accuracy de tu \n",
    "#modelo\n",
    "\n",
    "\n",
    "#Tip: Alabado sea SKlearn, con el nada te faltar√°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Embedded Method\n",
    "Son algoritmos que usan su propio criterio de selecci√≥n de variables. Ac√° encontramos algunos cl√°sicos como Lasso y Random Forest. En el caso de Lasso, este modelo generalmente fuerza a algunas variables a que tengan pesos equivalente a cero, dejando nulo el impacto de estas en la predicci√≥n. Por otro lado, el Random Forest una vez entrenado permite hacer un an√°lisis de importancia. Tambi√©n se puede usar GBM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Elige 1 criterios de los anteriores\n",
    "#2.Haz una peque√±a b√∫squeda bibliogr√°fica para definir con tus palabras en qu√© consiste\n",
    "#cada m√©todo. Si te sientes muy perdida pide referencias a la monitora.\n",
    "#3.Entrena el modelo seleccionado con la data pura\n",
    "#4.Entrena el modelo selccionado con alguna feature selection anterior.\n",
    "\n",
    "\n",
    "#Tip: Alabado sea SKlearn, con el nada te faltar√°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Conclusiones y tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completa esta secci√≥n con las cosas m√°s importantes \n",
    "#que aprendiste durante el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "ef640dac-464f-471b-8e7d-904f2a03df31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
